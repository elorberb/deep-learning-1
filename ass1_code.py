import numpy as np
import os
import matplotlib.pyplot as plt
import time
from sklearn.model_selection import train_test_split

np.random.seed(314977596)


def initialize_parameters(layer_dims: list) -> dict:
    """

    @param layer_dims:
    an array of the dimensions of each layer in the
    network (layer 0 is the size of the flattened input, layer L is the output softmax)
    @return:
    params: a dictionary containing the initialized W and b parameters of each layer (W1…WL, b1…bL).
    """
    params = {}
    # starting from 1 because index 0 is input
    for i in range(1, len(layer_dims)):
        Wi = np.random.randn(layer_dims[i], layer_dims[i - 1])  # define weights
        b = np.zeros((layer_dims[i], 1))  # define biases
        params[i] = [Wi, b]
    return params


def linear_forward(A: np.array, W: np.array, b: np.array) -> (float, tuple):
    """
    Description:
    Implement the linear part of a layer's forward propagation.

    @param A: the activations of the previous layer
    @param W: the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
    @param b: the bias vector of the current layer (of shape [size of current layer, 1])
    @return:
    Z: the linear component of the activation function (i.e., the value before applying the non-linear function)
    linear_cache: a dictionary containing A, W, b (stored for making the backpropagation easier to compute)

    """
    print("A", A.shape)
    print("W", W.shape)
    print("b", b)
    Z = np.dot(W, A) + b
    linear_cache = (A, W, b)
    return Z, linear_cache


def softmax(Z: np.array) -> (np.array, np.array):
    """

    @param Z: the linear component of the activation function
    @return:
    A: the activations of the layer
    activation_cache: returns Z, which will be useful for the backpropagation

    """
    A = np.exp(Z) / np.sum(np.exp(Z), axis=0)
    activation_cache = Z
    return A, activation_cache


def relu(Z: np.array) -> (np.array, tuple):
    """
    @param Z: the linear component of the activation function
    @return:
    A – the activations of the layer
    activation_cache – returns Z, which will be useful for the backpropagation
    """
    A = np.maximum(0, Z)
    activation_cache = Z
    return A, activation_cache


def linear_activation_forward(A_prev: np.array, W: np.array, B: np.array, activation: str) -> (float, tuple):
    """
    Description:
    Implement the forward propagation for the LINEAR->ACTIVATION layer

    @param A_prev: activations of the previous layer
    @param W: the weights matrix of the current layer
    @param B: the bias vector of the current layer
    @param activation: the activation function to be used (a string, either “softmax” or “relu”)
    @return:
    A: the activations of the current layer
    cache: a joint dictionary containing both linear_cache and activation_cache

    """
    Z, linear_cache = linear_forward(A_prev, W, B)
    A, activation_cache = None, None
    if activation == 'relu':
        A, activation_cache = relu(Z)
    elif activation == 'softmax':
        A, activation_cache = softmax(Z)
    cache = [linear_cache, activation_cache]
    return A, cache


def l_model_forward(X: np.array, parameters: dict, use_batchnorm: bool) -> (np.array, tuple):
    """
    Description:
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation

    @param X: the data, numpy array of shape (input size, number of examples)
    @param parameters: the initialized W and b parameters of each layer
    @param use_batchnorm: a boolean flag used to determine whether to apply batchnorm after
    the activation (note that this option needs to be set to “false” in Section 3 and “true” in Section 4).

    @return:
    AL – the last post-activation value
    caches – a list of all the cache objects generated by the linear_forward function

    """
    caches = []
    A = X
    layers_len = len(parameters)
    # Each middle layer - relu activations
    for i in range(1, layers_len):
        W, B = parameters[i][0], parameters[i][1]
        A, cache = linear_activation_forward(A_prev=A, W=W, B=B, activation="relu")
        if use_batchnorm:
            A = apply_batchnorm(A)
        caches.append(cache)
    # Last layer - softmax activation
    last_W, last_B = parameters[layers_len][0], parameters[layers_len][1]
    AL, cache = linear_activation_forward(A_prev=A, W=last_W, B=last_B, activation="softmax")
    caches.append(cache)

    return AL, caches


def compute_cost(AL: np.array, Y: np.array) -> np.array:
    """
    Description:
    Implement the cost function defined by equation. The requested cost function is categorical cross-entropy loss.

    @param AL: probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)
    @param Y: the labels vector (i.e. the ground truth)
    @return:
    cost: the cross-entropy cost
    """
    cost = 0
    num_of_classes = AL.shape[0]
    num_of_examples = AL.shape[1]
    for e in range(num_of_examples):
        for c in range(num_of_classes):
            print(Y[c][e])
            if Y[c][e] == 1:  # saves unnecessary computation
                cost -= Y[c][e] * np.log(AL[c][e])
    cost = cost / num_of_examples
    return cost


def apply_batchnorm(A: np.array):
    """
    Description:
    performs batchnorm on the received activation values of a given layer.

    @param A: the activation values of a given layer
    @return:
    NA: the normalized activation values, based on the formula learned in class
    """

    mean_values = np.mean(A)
    var_values = np.var(A)
    eps = np.finfo(float).eps
    NA = (A - mean_values) / np.sqrt((var_values + eps))
    return NA


def linear_backward(dZ, cache):
    """
    description:
    Implements the linear part of the backward propagation process for a single layer

    @param dZ: the gradient of the cost with respect to the linear output of the current layer (layer l)
    @param cache: tuple of values (A_prev, W, b) coming from the forward propagation in the current layer
    @return:
    dA_prev: Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW: Gradient of the cost with respect to W (current layer l), same shape as W
    db: Gradient of the cost with respect to b (current layer l), same shape as b

    """
    A_prev, W, b = cache
    len_examples = A_prev.shape[1]

    dA_prev = np.dot(W.T, dZ)
    dW = np.dot(dZ, A_prev.T) / len_examples
    db = np.sum(dZ, axis=1, keepdims=True) / len_examples

    return dA_prev, dW, db


def linear_activation_backward(dA, cache, activation):
    """
    Description:
    Implements the backward propagation for the LINEAR->ACTIVATION layer.
    The function first computes dZ and then applies the linear_backward function.

    @param dA: post activation gradient of the current layer
    @param cache: contains both the linear cache and the activations cache
    @param activation: which activation function used to compute the A
    @return:
    dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW – Gradient of the cost with respect to W (current layer l), same shape as W
    db – Gradient of the cost with respect to b (current layer l), same shape as b

    """
    dZ = None
    linear_cache, activation_cache = cache

    if activation == 'relu':
        dZ = relu_backward(dA, activation_cache)
    elif activation == 'softmax':
        dZ = softmax_backward(dA, activation_cache)

    dA_prev, dW, db = linear_backward(dZ, linear_cache)
    return dA_prev, dW, db


def relu_backward(dA, activation_cache):
    """
    Description:
    Implements backward propagation for a ReLU unit

    @param dA: the post-activation gradient
    @param activation_cache: contains Z (stored during the forward propagation)
    @return:
    dZ: gradient of the cost with respect to Z
    """

    dZ = np.array(dA, copy=True)
    dZ[activation_cache <= 0] = 0
    dZ[activation_cache > 0] = 1
    return dZ


def softmax_backward(dA, activation_cache):
    """
    Description:
    Implements backward propagation for a softmax unit

    @param dA: the post-activation gradient - p - output of the softmax forward propagation
    @param activation_cache: contains Z (stored during the forward propagation) - Y - true value
    @return:
    dZ: gradient of the cost with respect to Z
    """
    # correct params
    Y = activation_cache
    p = dA
    dZ = p - Y
    return dZ


def l_model_backward(AL, Y, caches):
    """
    Description:
    Implement the backward propagation process for the entire network.

    @param AL: the probabilities vector, the output of the forward propagation (L_model_forward)
    @param Y: the true labels vector (the "ground truth" - true classifications)
    @param caches: list of caches containing for each layer: a) the linear cache; b) the activation cache
    @return:
    Grads - a dictionary with the gradients
    """
    grads = {}
    num_layers = len(caches)
    last_cache = caches[-1]
    dZ = softmax_backward(dA=AL, activation_cache=Y.reshape(AL.shape))
    dA, dW, db = linear_backward(dZ=dZ, cache=last_cache)
    grads[f"dA+{num_layers}"], grads[f"dW+{num_layers}"], grads[f"db+{num_layers}"] = dA, dW, db

    for curr_layer in reversed(range(num_layers - 1)):
        dA, dW, db = linear_activation_backward(grads[f"dA{curr_layer+2}"], caches[curr_layer], "relu")
        grads[f"dA+{curr_layer+1}"], grads[f"dW+{curr_layer+1}"], grads[f"db+{curr_layer+1}"] = dA, dW, db

    return grads


def update_parameters(parameters, grads, learning_rate):
    """
    Description:
    Updates parameters using gradient descent

    @param parameters: a python dictionary containing the DNN architecture’s parameters
    @param grads: a python dictionary containing the gradients (generated by L_model_backward)
    @param learning_rate: the learning rate used to update the parameters (the “alpha”)
    @return:
    parameters – the updated values of the parameters object provided as input
    """
    num_layers = len(parameters)
    for i in range(num_layers+1):
        parameters[i][0] -= learning_rate * grads[f"dW+{i}"]
        parameters[i][1] -= learning_rate * grads[f"db+{i}"]

    return parameters


def l_layer_model(X, Y, layers_dims, learning_rate, num_iterations, batch_size):
    pass


def predict(X, Y, parameters, use_batchnorm):
    pass
